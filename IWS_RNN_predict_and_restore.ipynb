{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IWS Module1 Fianl_v2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPViO6WyJ8sy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d602c68-ebfc-4514-8a12-bc52620f03ba"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/gdrive')\n",
        "os.chdir(\"/gdrive/My Drive/MLcontent/\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCu4j9ZSMtut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p my_models"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wis4B3cCJ4dt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "e2bbc892-7239-4b79-f13c-78ef1d207aa2"
      },
      "source": [
        "from pandas import read_csv\n",
        "from pandas import datetime\n",
        "from pandas import to_numeric\n",
        "from pandas import concat\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "#tf.enable_v2_behavior()\n",
        "from tensorflow.compat.v1.keras import backend as K\n",
        "from keras.layers import SimpleRNN, Dense, LSTM, Bidirectional, GRU\n",
        "from keras.models import Sequential\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import os\n",
        "import random as rn\n",
        "#from keras import backend as K\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "#Setting a seed for the computer's pseudorandom number generator. \n",
        "#This allows us to reproduce the results from our script:\n",
        "n = 5\n",
        "np.random.seed(100 * n)\n",
        "rn.seed(10000 * n)\n",
        "\n",
        "#Depending on the actual running environment, you may specify if using GPU, or only CPU.\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "#Tensorflow session configuration.\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "\n",
        "tf.compat.v1.set_random_seed(1000 * n)\n",
        "\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n",
        "\n",
        "series = read_csv('Wastewater_Data.csv', header=0, parse_dates=[0], index_col=0, squeeze=True)\n",
        "series = series.replace('^\\s*$', np.nan, regex=True)\n",
        "series = series.fillna(method='ffill')\n",
        "series = series.apply(to_numeric)\n",
        "\n",
        "#lag defines how many historical data are used to predict a specific wasterwater characteristic (e.g., BOD5)\n",
        "lag = 7\n",
        "\n",
        "#num_features defines how many historical wasterwater characteristics are used. We have 9 in total (i.e., TS, BOD5, NH3, etc.)\n",
        "num_features = 9\n",
        "target_dict = {}\n",
        "for i, j in enumerate(series.columns):\n",
        "    target_dict[j] = i\n",
        "\n",
        "#print(target_dict['Total Solids'])\n",
        "from pandas import DataFrame\n",
        "from sklearn import preprocessing\n",
        "names = series.columns\n",
        "x = series.values #returns a numpy array\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit(x)\n",
        "Maxdata = list(x_scaled.data_max_)\n",
        "Mindata = list(x_scaled.data_min_)\n",
        "x_scaled = min_max_scaler.transform(x)\n",
        "series_normalized = DataFrame(x_scaled, columns=names) \n",
        "\n",
        "\n",
        "\n",
        "#The following line of code will do the same normalization as the code above.\n",
        "#series_normalized = (series - np.min(series))/(np.max(series)-np.min(series))\n",
        "\n",
        "# table2lags() Shifts a dataFrame along its time axis (i.e., index) n steps \n",
        "# (moving down/up if step is a positive/negative number), determined by min_lag and max_lag, \n",
        "# and merge all shiffted dataframes into a single one and return, \n",
        "# without including the original DataFrame (the one shifted 0 step).\n",
        "# \"values\" is a list, and each item in \"values\" is a shifted dataframe.\n",
        "# Input: Table: a DataFrame; max_lag: the maximum shifting; min_lag: the minimum shifting; \n",
        "# In this module, no need to include the original dataframe or shift the dataframe up, and thus min_lag = 1 by default\n",
        "# separator: used to concatenate the step value (e.g., 1, 2, 3) to the original column label. E.g., SO4_1, SO4_2, etc.\n",
        "# Output: a dataframe\n",
        "\n",
        "def table2lags(table, max_lag, min_lag=1, separator='_'):\n",
        "    values = []\n",
        "    for i in range(min_lag, max_lag + 1):\n",
        "        #append shiffted dataframe into the list (i.e., values)\n",
        "        values.append(table.shift(i).copy()) \n",
        "        #replace the last item or dataframe's columns by column_n; n is the shift step  \n",
        "        values[-1].columns = [c + separator + str(i) for c in table.columns] \n",
        "    #pandas.concat is used to merge all dataframes (as items in values) into a single dataframe\n",
        "    return concat(values, axis=1) \n",
        "\n",
        "#prepare all historical (e.g., one day ago, two day ago,...) data into one dataframe. \n",
        "#For all missing data, replaced by its closest future values\n",
        "X = table2lags(series_normalized, lag)\n",
        "X = X.fillna(method='bfill')\n",
        "print(X.columns)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n",
            "Index(['Total Solids_1', 'SS_1', 'BOD5_1', 'NH3_1', 'Org-N_1', 'P-TOT_1',\n",
            "       'SO4_1', 'TKN_1', 'PRCP_NOOA_1', 'Total Solids_2', 'SS_2', 'BOD5_2',\n",
            "       'NH3_2', 'Org-N_2', 'P-TOT_2', 'SO4_2', 'TKN_2', 'PRCP_NOOA_2',\n",
            "       'Total Solids_3', 'SS_3', 'BOD5_3', 'NH3_3', 'Org-N_3', 'P-TOT_3',\n",
            "       'SO4_3', 'TKN_3', 'PRCP_NOOA_3', 'Total Solids_4', 'SS_4', 'BOD5_4',\n",
            "       'NH3_4', 'Org-N_4', 'P-TOT_4', 'SO4_4', 'TKN_4', 'PRCP_NOOA_4',\n",
            "       'Total Solids_5', 'SS_5', 'BOD5_5', 'NH3_5', 'Org-N_5', 'P-TOT_5',\n",
            "       'SO4_5', 'TKN_5', 'PRCP_NOOA_5', 'Total Solids_6', 'SS_6', 'BOD5_6',\n",
            "       'NH3_6', 'Org-N_6', 'P-TOT_6', 'SO4_6', 'TKN_6', 'PRCP_NOOA_6',\n",
            "       'Total Solids_7', 'SS_7', 'BOD5_7', 'NH3_7', 'Org-N_7', 'P-TOT_7',\n",
            "       'SO4_7', 'TKN_7', 'PRCP_NOOA_7'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao6Rgzm5W_HL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# targetList = [\"Total Solids\",\t\"SS\", \t\"BOD5\",\t\"NH3\",\t\"Org-N\",\t\"P-TOT\",\t\"SO4\",\t\"TKN\",\t\"PRCP_NOOA\"]\n",
        "# for name in targetList:\n",
        "#     targetDF = series_normalized[name].to_frame()\n",
        "#     #print(X_test)\n",
        "#     size = int(len(X) * 0.8) #split the data into training set (80%) and validation set\n",
        "#     train = X[0:size]\n",
        "#     train_target = targetDF[0:size]\n",
        "#     test = X[size:len(X)]\n",
        "#     test_target = targetDF[size:len(X)]\n",
        "\n",
        "#     X_train = train.values.reshape(-1, lag, num_features).astype('float32')\n",
        "#     y_train = train_target.values.astype('float32')\n",
        "\n",
        "#     X_test = test.values.reshape(-1, lag, num_features).astype('float32')\n",
        "#     y_test = test_target.values.astype('float32')\n",
        "#     print(X_test)\n",
        "#     hidden = 64\n",
        "#     batch_size = 20\n",
        "#     epochs = 17\n",
        "\n",
        "#     #we'll import the Sequential model type from Keras. \n",
        "#     #This is simply a linear stack of neural network layers, and it's perfect for the type of feed-forward CNN and RNN\n",
        "#     model1 = Sequential()\n",
        "#     model1.add(Bidirectional(LSTM(hidden), input_shape=(lag, num_features)))\n",
        "#     model1.add(Dense(1))\n",
        "\n",
        "#     model1.compile(optimizer='adam',\n",
        "#                 loss='mae',\n",
        "#                 metrics=['mae'])\n",
        "\n",
        "\n",
        "#     history = model1.fit(X_train, y_train,\n",
        "#                         epochs=epochs,\n",
        "#                         batch_size=batch_size,\n",
        "#                         validation_data=(X_test,y_test))\n",
        "#     model1.save('my_models/myIWS_RNNmodel' + str(name))\n",
        "#     y_predict = model1.predict(X_test)\n",
        "#     plt.style.use(\"ggplot\")\n",
        "\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "#     plt.plot(y_test, color='black', linewidth=.2, marker='o', markersize=2,\n",
        "#                 markeredgecolor='black', markeredgewidth=0.2, fillstyle='none')\n",
        "#     plt.plot(y_predict, color='blue', linewidth=4, linestyle=' ', marker='x', markersize=4,\n",
        "#                 markeredgecolor='blue', markeredgewidth=.2, fillstyle='none')\n",
        "#     plt.legend(('Test Data', 'Predictions'))\n",
        "#     #plt.savefig(name+\".png\", dpi = 600)\n",
        "#     plt.ylabel(name)\n",
        "\n",
        "#     plt.show()\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCHFXo4SIbj5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "620f0ecd-a1bc-4401-88c8-2c185413c430"
      },
      "source": [
        "#input start and end date(M/D/Y), target\n",
        "#output prediction, real value, error.\n",
        "import pandas as pd\n",
        "output_targets = [\"Total Solids\", \"SS\"]\n",
        "start_date = \"12/28/2018\"\n",
        "end_date = \"01/01/2019\"\n",
        "\n",
        "timelist = pd.date_range('1/1/2001','12/31/2018')\n",
        "input_datasetdf = pd.DataFrame(X.values, index=timelist)[start_date: end_date]\n",
        "input_dataset = input_datasetdf.values.reshape(-1, lag, num_features).astype('float32')\n",
        "#Error mae mape\n",
        "def mae_mape(actual, pred):\n",
        "    actual, pred = np.array(actual), np.array(pred)\n",
        "    return np.abs(actual - pred), np.mean(np.abs(actual - pred)), np.mean(np.abs(actual - pred) / actual)\n",
        "\n",
        "#return a dictionary\n",
        "#list predict values,list actual values, array error, mae, mape no %\n",
        "def predictWithModel(start_date=start_date, end_date=end_date, output_targets=output_targets, input_dataset=input_dataset, realdata=series, Maxdata=Maxdata, Mindata=Mindata):\n",
        "    ansdict = {}\n",
        "    for target in output_targets: \n",
        "        model_using = tf.keras.models.load_model('my_models/myIWS_RNNmodel' + str(target))\n",
        "        y_predict = model_using.predict(input_dataset)\n",
        "        origin_predict = []\n",
        "        for ans in y_predict:\n",
        "            origin_predict.append(ans[0] * (Maxdata[target_dict[str(target)]] - Mindata[target_dict[str(target)]]) +  Mindata[target_dict[str(target)]])\n",
        "        #print(origin_predict)\n",
        "        #print(list(realdata[start_date: end_date][str(target)].values))\n",
        "        error, mae, mape= mae_mape(origin_predict, list(realdata[start_date: end_date][str(target)].values))\n",
        "        ansdict[str(target)] = (origin_predict, list(realdata[start_date: end_date][str(target)].values), error, mae, mape)\n",
        "    return ansdict\n",
        "t = predictWithModel()\n",
        "print(t)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[837.3319984674454, 935.9792563319206, 965.1203734874725, 977.9081639349461]\n",
            "[776.0, 898.0, 878.0, 920.0]\n",
            "[151.4605889096856, 151.64912625402212, 152.54022578895092, 157.33025784045458]\n",
            "[104.0, 148.0, 124.0, 96.0]\n",
            "{'Total Solids': ([837.3319984674454, 935.9792563319206, 965.1203734874725, 977.9081639349461], [776.0, 898.0, 878.0, 920.0], array([61.33199847, 37.97925633, 87.12037349, 57.90816393]), 61.08494805544615, 0.06582730996146648), 'SS': ([151.4605889096856, 151.64912625402212, 152.54022578895092, 157.33025784045458], [104.0, 148.0, 124.0, 96.0], array([47.46058891,  3.64912625, 28.54022579, 61.33025784]), 35.24504969827831, 0.22858348384288057)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAMN0k26IMY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}